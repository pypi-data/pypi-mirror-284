Metadata-Version: 2.1
Name: timestep
Version: 10.0.31
Summary: 
License: MIT
Author: Michael James Schock
Author-email: m@mjschock.com
Requires-Python: >=3.10,<4.0
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Dist: connexion[flask,mock,swagger-ui,uvicorn] (>=3.1.0,<4.0.0)
Requires-Dist: controlflow (>=0.8.2,<0.9.0)
Requires-Dist: flask (>=3.0.3,<4.0.0)
Requires-Dist: huggingface-hub (>=0.23.4,<0.24.0)
Requires-Dist: instructor (>=1.3.4,<2.0.0)
Requires-Dist: llama-cpp-python[server] (>=0.2.82,<0.3.0)
Requires-Dist: marvin (>=2.3.6,<3.0.0)
Requires-Dist: openai (>=1.35.10,<2.0.0)
Requires-Dist: prefect-shell (>=0.3.0rc1,<0.4.0)
Requires-Dist: prefect-sqlalchemy (>=0.5.0rc2,<0.6.0)
Requires-Dist: python-jose[cryptography] (>=3.3.0,<4.0.0)
Requires-Dist: sse-starlette (>=2.1.2,<3.0.0)
Requires-Dist: tiktoken (>=0.7.0,<0.8.0)
Requires-Dist: transformers (>=4.42.4,<5.0.0)
Requires-Dist: typer[all] (>=0.12.3,<0.13.0)
Description-Content-Type: text/markdown

# `timestep`

Timestep AI CLI

**Usage**:

```console
$ timestep [OPTIONS] COMMAND [ARGS]...
```

**Options**:

* `--install-completion`: Install completion for the current shell.
* `--show-completion`: Show completion for the current shell, to copy it or customize the installation.
* `--help`: Show this message and exit.

**Commands**:

* `launch`: Launch
* `llamafile`
* `serve`: Serve
* `test`: Test
* `train`: Train

## `timestep launch`

Launch

**Usage**:

```console
$ timestep launch [OPTIONS]
```

**Options**:

* `--help`: Show this message and exit.

## `timestep llamafile`

**Usage**:

```console
$ timestep llamafile [OPTIONS] COMMAND [ARGS]...
```

**Options**:

* `--help`: Show this message and exit.

**Commands**:

* `load`: Load a model
* `unload`: Unload a model by PID

### `timestep llamafile load`

Load a model

**Usage**:

```console
$ timestep llamafile load [OPTIONS]
```

**Options**:

* `--llamafile-path TEXT`: [default: ./models/TinyLlama-1.1B-Chat-v1.0.F16.llamafile]
* `--host TEXT`: [default: 0.0.0.0]
* `--public-path TEXT`: [default: /zip/llama.cpp/server/public]
* `--port TEXT`: [default: 8080]
* `--help`: Show this message and exit.

### `timestep llamafile unload`

Unload a model by PID

**Usage**:

```console
$ timestep llamafile unload [OPTIONS] PID
```

**Arguments**:

* `PID`: [required]

**Options**:

* `--help`: Show this message and exit.

## `timestep serve`

Serve

**Usage**:

```console
$ timestep serve [OPTIONS]
```

**Options**:

* `--help`: Show this message and exit.

## `timestep test`

Test

**Usage**:

```console
$ timestep test [OPTIONS]
```

**Options**:

* `--api-key TEXT`: [default: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJjb20uemFsYW5kby5jb25uZXhpb24iLCJpYXQiOjE3MjA3MzQ0NTgsImV4cCI6MTcyMDczNTA1OCwic3ViIjoiNDcifQ.1dmckj48HdxUb2uAyvrdeRHbLZQkEUKdQClSCtpKtQ0]
* `--base-url TEXT`: [default: http://localhost:8000/api/openai/v1]
* `--message TEXT`: [default: Count to 10, with a comma between each number and no newlines. E.g., 1, 2, 3, ...]
* `--stream / --no-stream`: [default: stream]
* `--help`: Show this message and exit.

## `timestep train`

Train

**Usage**:

```console
$ timestep train [OPTIONS]
```

**Options**:

* `--help`: Show this message and exit.

