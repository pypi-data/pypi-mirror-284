{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ezlocalai\n",
    "\n",
    "Open this notebook in [Google Colab.](https://colab.research.google.com/)\n",
    "\n",
    "Set Runtime Python 3 with T4 GPU or higher.\n",
    "\n",
    "It takes several minutes to install dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install -y portaudio19-dev ffmpeg libportaudio2 libasound-dev\n",
    "!pip install -qq scipy ftfy accelerate fastapi uvicorn pydantic requests tiktoken python-dotenv beautifulsoup4 faster-whisper pydub ffmpeg TTS sounddevice pyaudio webrtcvad pyngrok pdfplumber spacy python-multipart\n",
    "!pip install -qq diffusers[\"torch\"] transformers torch torchvision torchaudio\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
    "!rm -rf sample_data .config\n",
    "!git clone https://github.com/DevXT-LLC/ezlocalai ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the server\n",
    "\n",
    "Set the `NGROK_TOKEN` to use NGROK to expose your ezlocalai server to the public with as simple as an API key. [Get your free NGROK_TOKEN here.](https://dashboard.ngrok.com/get-started/your-authtoken)\n",
    "\n",
    "You can see the ngrok URL in the cell output after running the cell.\n",
    "\n",
    "Set the `LLM_TO_USE` with the name of the model from the models list or from the [Hugging Face model directory of GGUF Quantized Models](https://huggingface.co/models?search=GGUF). You won't have to download the model, just enter its path. We will use `MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF` for this example since it is a smaller model that can still be useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_TO_USE = \"MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF\"\n",
    "VISION_MODEL = \"deepseek-ai/deepseek-vl-1.3b-chat\"\n",
    "GPU_LAYERS = 20\n",
    "LLM_MAX_TOKENS = 8192\n",
    "\n",
    "# Add your NGROK_TOKEN to your colab secrets if using Google Colab (Key logo on the left)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    NGROK_TOKEN = userdata.get('NGROK_TOKEN')\n",
    "    if not NGROK_TOKEN:\n",
    "        raise\n",
    "except:\n",
    "    # If you're not using Google Colab, enter your NGROK_TOKEN below.\n",
    "    NGROK_TOKEN = \"Enter your ngrok token here\"\n",
    "if NGROK_TOKEN != \"Enter your ngrok token here\":\n",
    "    with open('.env', 'r') as file:\n",
    "        filedata = file.read()\n",
    "    filedata = filedata.replace('NGROK_TOKEN=\\n', f'NGROK_TOKEN={NGROK_TOKEN}\\n')\n",
    "    filedata = filedata.replace('DEFAULT_MODEL=TheBloke/phi-2-dpo-GGUF', f'DEFAULT_MODEL={LLM_TO_USE}')\n",
    "    filedata = filedata.replace('GPU_LAYERS=0', f'GPU_LAYERS={GPU_LAYERS}')\n",
    "    filedata = filedata.replace('LLM_MAX_TOKENS=0', f'LLM_MAX_TOKENS={LLM_MAX_TOKENS}')\n",
    "    filedata = filedata.replace('VISION_MODEL=deepseek-ai/deepseek-vl-1.3b-chat', f'VISION_MODEL={VISION_MODEL}')\n",
    "    with open('.env', 'w') as file:\n",
    "        file.write(filedata)\n",
    "!uvicorn app:app --host 0.0.0.0 --port 8091 --workers 1 --proxy-headers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
