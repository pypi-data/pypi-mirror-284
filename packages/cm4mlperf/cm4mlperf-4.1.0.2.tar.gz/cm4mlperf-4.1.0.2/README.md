# CM interface to run MLPerf inference benchmarks

Install the [CM automation framework](https://github.com/mlcommons/ck) as described [here](https://access.cknowledge.org/playground/?action=install).

Follow [these instructions](https://docs.mlcommons.org/inference) to run MLPerf inference benchmarks using the CM interface.

# Acknowledgments

This project is sponsored by [MLCommons](https://mlcommons.org), [cTuning foundation](https://cTuning.org) and [cKnowledge](https://cKnowledge.org).

You can site this automation project using [this article](http://arxiv.org/abs/2406.16791):
```
@misc{fursin2024enabling,
      title={Enabling more efficient and cost-effective AI/ML systems with Collective Mind, virtualized MLOps, MLPerf, Collective Knowledge Playground and reproducible optimization tournaments}, 
      author={Grigori Fursin},
      year={2024},
      eprint={2406.16791},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}
```

You can learn more about the MLPerf inference benchmark [here](https://arxiv.org/abs/1911.02549).
