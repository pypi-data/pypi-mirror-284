{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this notebook we demonstrate the how to train the MixMIL model on data simulated under as specified in the paper in the Binomial likelihood setting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/alessandro.palma/miniconda3/envs/sslbio-env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from mixmil import MixMIL\n",
    "from mixmil.data import load_data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy.stats as st\n",
    "\n",
    "def _calc_metrics(u_pred, w_pred, u, w):\n",
    "    \"\"\"\n",
    "    Calculate correlation and AUC metrics using real and predicted instance weights.\n",
    "\n",
    "    Parameters:\n",
    "    - u_pred (numpy.ndarray): Predicted instance-level weights.\n",
    "    - w_pred (numpy.ndarray): Predicted instance-level weights as instance proportions.\n",
    "    - u (numpy.ndarray): True instance-level weights.\n",
    "    - w (numpy.ndarray): True instance-level weights as instance proportions.\n",
    "\n",
    "    Returns:\n",
    "    - rho_bag (float): Weight correlation (Spearman's rank correlation coefficient).\n",
    "    - auc_instance (float): Instance retrieval AUC (Area Under the Receiver Operating Characteristic curve).\n",
    "    \"\"\"\n",
    "    rho_bag = st.spearmanr(u_pred, u).correlation  # bag level correlation\n",
    "    is_top_instance = (w > np.quantile(w, 0.90)).long().ravel()\n",
    "    auc_instance = roc_auc_score(is_top_instance, w_pred)  # instance-retrieval AUC\n",
    "    return rho_bag, auc_instance\n",
    "\n",
    "\n",
    "def calc_metrics(model, X, u, w):\n",
    "    \"\"\"\n",
    "    Calculate aggregated metrics over multiple bags or instances for a given model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model.\n",
    "    - X (dict): Dictionary containing input data.\n",
    "    - u (dict): Dictionary containing true values for instance-level weights.\n",
    "    - w (dict): Dictionary containing true values for instance-level weights as proportions.\n",
    "\n",
    "    Returns:\n",
    "    - res_dict (dict): Dictionary containing aggregated metrics including:\n",
    "        - 'rho_bag' (float): Mean bag-level weight correlation.\n",
    "        - 'rho_bag_err' (float): Standard error of the mean for bag-level correlation.\n",
    "        - 'auc_instance' (float): Mean instance retrieval AUC.\n",
    "        - 'auc_instance_err' (float): Standard error of the mean for instance retrieval AUC.\n",
    "    \"\"\"\n",
    "    u_pred = model.predict(X[\"test\"]).cpu().numpy()\n",
    "    w_pred = model.get_weights(X[\"test\"])[0].cpu().numpy()\n",
    "\n",
    "    P = u_pred.shape[1]\n",
    "    if P > 0:\n",
    "        rho_bag, auc_instance = [], []\n",
    "        for i in range(P):\n",
    "            _rho_bag, _auc_instance = _calc_metrics(\n",
    "                u_pred[..., i], w_pred[..., i].ravel(), u[\"test\"][..., i], w[\"test\"][..., i].ravel()\n",
    "            )\n",
    "            rho_bag.append(_rho_bag)\n",
    "            auc_instance.append(_auc_instance)\n",
    "\n",
    "        res_dict = {\n",
    "            \"rho_bag\": np.mean(rho_bag),\n",
    "            \"rho_bag_err\": np.std(rho_bag) / np.sqrt(P),\n",
    "            \"auc_instance\": np.mean(auc_instance),\n",
    "            \"auc_instance_err\": np.std(auc_instance) / np.sqrt(P),\n",
    "        }\n",
    "    else:\n",
    "        rho_bag, auc_instance = _calc_metrics(u_pred, w_pred.ravel(), u[\"test\"], w[\"test\"].ravel())\n",
    "        res_dict = {\"rho_bag\": rho_bag, \"auc_instance\": auc_instance}\n",
    "    return res_dict\n",
    "\n",
    "def print_metrics(prefix, metrics):\n",
    "    \"\"\"\n",
    "    Print a formatted representation of metrics with a specified prefix.\n",
    "\n",
    "    Parameters:\n",
    "    - prefix (str): Prefix to be added to the printed metrics, for better identification.\n",
    "    - metrics (dict): Dictionary containing metrics to be printed.\n",
    "\n",
    "    Returns:\n",
    "    - None: This function prints the metrics to the console without returning any value.\n",
    "    \"\"\"\n",
    "    print(f\"{prefix} metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model with simulated data under using a binomial likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLMM Init: 100%|██████████| 10/10 [00:07<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] metrics:\n",
      "rho_bag: 0.6004\n",
      "rho_bag_err: 0.0131\n",
      "auc_instance: 0.5000\n",
      "auc_instance_err: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 2000/2000 [04:30<00:00,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[END] metrics:\n",
      "rho_bag: 0.8316\n",
      "rho_bag_err: 0.0111\n",
      "auc_instance: 0.9362\n",
      "auc_instance_err: 0.0078\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Simulate data as described in the paper\n",
    "# embeddings, fixed effects, labels, sim bag predictions, sim instance weights\n",
    "# P: number of outputs, simulated from the same embeddings X\n",
    "X, F, Y, u, w = load_data(P=10, seed=0)\n",
    "model = MixMIL.init_with_mean_model(X[\"train\"], F[\"train\"], Y[\"train\"], likelihood=\"binomial\", n_trials=2).to(device)\n",
    "X, F, Y = [{key: val.to(device) for key, val in el.items()} for el in [X, F, Y]]\n",
    "\n",
    "print_metrics(\"[START]\", calc_metrics(model, X, u, w))\n",
    "# Fit model in parallel to each output separately\n",
    "model.train(X[\"train\"], F[\"train\"], Y[\"train\"], n_epochs=2_000)\n",
    "print_metrics(\"[END]\", calc_metrics(model, X, u, w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
