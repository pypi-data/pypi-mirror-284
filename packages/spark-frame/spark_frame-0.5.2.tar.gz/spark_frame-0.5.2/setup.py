# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['spark_frame',
 'spark_frame.data_diff',
 'spark_frame.examples',
 'spark_frame.fp',
 'spark_frame.graph_impl',
 'spark_frame.nested_functions_impl',
 'spark_frame.nested_impl',
 'spark_frame.transformations_impl']

package_data = \
{'': ['*']}

install_requires = \
['pip>=24.0.0,<25.0.0']

setup_kwargs = {
    'name': 'spark-frame',
    'version': '0.5.2',
    'description': 'A library containing various utility functions for playing with PySpark DataFrames',
    'long_description': '# Spark-frame\n\n[![PyPI version](https://badge.fury.io/py/spark-frame.svg)](https://badge.fury.io/py/spark-frame)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/spark-frame.svg)](https://pypi.org/project/spark-frame/)\n[![GitHub Build](https://img.shields.io/github/actions/workflow/status/FurcyPin/spark-frame/build_and_validate.yml?branch=main)](https://github.com/FurcyPin/spark-frame/actions)\n[![SonarCloud Coverage](https://sonarcloud.io/api/project_badges/measure?project=FurcyPin_spark-frame&metric=coverage)](https://sonarcloud.io/component_measures?id=FurcyPin_spark-frame&metric=coverage&view=list)\n[![SonarCloud Bugs](https://sonarcloud.io/api/project_badges/measure?project=FurcyPin_spark-frame&metric=bugs)](https://sonarcloud.io/component_measures?metric=reliability_rating&view=list&id=FurcyPin_spark-frame)\n[![SonarCloud Vulnerabilities](https://sonarcloud.io/api/project_badges/measure?project=FurcyPin_spark-frame&metric=vulnerabilities)](https://sonarcloud.io/component_measures?metric=security_rating&view=list&id=FurcyPin_spark-frame)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/spark-frame)](https://pypi.org/project/spark-frame/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n\n## What is it ?\n\n**[Spark-frame](https://furcypin.github.io/spark-frame/) is a library that super-charges your Spark DataFrames!**\n\nIt brings several utility methods and transformation functions for PySpark DataFrames.\n\nHere is a quick list of the most exciting features :sunglasses:\n\n- `spark_frame.data_diff.compare_dataframes`: compare two SQL tables or DataFrames and generate an HTML report \n  to view the result. And yes, this is completely free, open source, and it works even with \n  complex data structures ! It also detects column reordering and can handle type changes.\n  [Go check it out :exclamation:](https://furcypin.github.io/spark-frame/use_cases/comparing_dataframes/)\n- `spark_frame.nested`: Did you ever thought manipulating complex data structures in SQL or Spark was a \n  nightmare :jack_o_lantern: ? You just found the solution ! The `nested` library  makes those manipulations much \n  cleaner and simpler. \n  [Get started over there :rocket:](https://furcypin.github.io/spark-frame/use_cases/working_with_nested_data/)\n- `spark_frame.transformations`: A wide collection of generic dataframe transformations.\n    - Ever wanted to apply a transformation to every field of a DataFrame depending on it\'s name or type ? \n      [Easy as pie :cake:](https://furcypin.github.io/spark-frame/reference/transformations/#spark_frame.transformations_impl.transform_all_fields.transform_all_fields)\n    - Ever wanted to rename every field of a DataFrame, including the deeply nested ones ? \n      [Done: :ok_hand:](https://furcypin.github.io/spark-frame/reference/transformations/#spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names)\n    - Ever wanted to analyze the content of a DataFrame, \n      but [`DataFrame.describe()`](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.describe.html)\n      does not work with complex data types ? \n      [You\'re welcome :pray:](https://furcypin.github.io/spark-frame/reference/transformations/#spark_frame.transformations_impl.analyze.analyze)\n- `spark_frame.schema_utils`: Need to dump the schema of a DataFrame somewhere to be able to load it later ?\n     [We got you covered :thumbsup:](https://furcypin.github.io/spark-frame/reference/schema_utils/)\n- `spark_frame.graph.ascending_forest_traversal`: Need an algorithm that takes the adjacency matrix of a \n tree :deciduous_tree: (or forest) graph and associates each node to their corresponding root node ?\n But that other algorithm you tried went into an infinite loop âˆž because your graph isn\'t really a tree \n and occasionally contains cycles ? \n [Try this :evergreen_tree:](https://furcypin.github.io/spark-frame/reference/graph/#spark_frame.graph_impl.ascending_forest_traversal.ascending_forest_traversal)\n\n\n## Getting Started\n\nVisit the official Spark-frame website [documentation](https://furcypin.github.io/spark-frame/) \nfor [use cases examples](https://furcypin.github.io/spark-frame/use_cases/intro/) \nand [reference](https://furcypin.github.io/spark-frame/reference/functions/).\n\n## Installation\n\n[spark-frame is available on PyPi](https://pypi.org/project/spark-frame/).\n\n```bash\npip install spark-frame\n```\n\n## Compatibilities and requirements\n\n\nThis library does not depend on any other library.\n**Pyspark must be installed separately to use it.**\nIt is compatible with the following versions:\n\n- Python: requires 3.8.1 or higher (tested against Python 3.8, 3.9, 3.10, 3.11 and 3.12)\n- Pyspark: requires 3.3.0 or higher (tested against PySpark 3.3, 3.4 and 3.5)\n\nThis library is tested against Windows, Mac and Linux.\n\nHowever, testing for the following combinations has been disabled, because they are failing:\n\n- PySpark 3.3 with Python >= 3.11\n- PySpark 3.4 with Python >= 3.12\n- PySpark 3.5 with Python 3.12 on Windows\n\n\n**Some features require extra libraries to be installed alongside this project.**\n**We chose to not include them as direct dependencies for security and flexibility reasons.**\n**This way, users who are not using these features don\'t need to worry about these dependencies.**\n\n| feature                                    |  Method                      | spark-frame\'s <br> version |               dependency required |\n|--------------------------------------------|------------------------------|----------------------------|----------------------------------:|\n| Generating HTML <br> reports for data diff |  `DiffResult.export_to_html` | >= 0.5.0                   | data-diff-viewer==0.3.* (>=0.3.2) |\n| Generating HTML <br> reports for data diff |  `DiffResult.export_to_html` | 0.4.*                      |           data-diff-viewer==0.2.* |\n| Generating HTML <br> reports for data diff |  `DiffResult.export_to_html` | < 0.4                      |                            jinja2 |\n\n_Since version 0.4, the code used to generate HTML diff reports has been moved to \n[data-diff-viewer](https://github.com/FurcyPin/data-diff-viewer) from the same author. \nIt comes with a dependency to [duckdb](https://github.com/duckdb/duckdb), \nwhich is used to store the diff results and embed them in the HTML page._\n\n\n# Genesis of the project\n\nThese methods were initially part of the [karadoc](https://github.com/FurcyPin/karadoc) project \nused at [Younited](https://medium.com/younited-tech-blog), but they were fully independent from karadoc, \nso it made more sense to keep them as a standalone library.\n\nSeveral of these methods were my initial inspiration to make the cousin project\n[bigquery-frame](https://github.com/FurcyPin/bigquery-frame), which was first made to illustrate\nthis [blog article](https://medium.com/towards-data-science/sql-jinja-is-not-enough-why-we-need-dataframes-4d71a191936d).\nThis is why you will find similar methods in both `spark_frame` and `bigquery_frame`, \nexcept the former runs on PySpark while the latter runs on BigQuery (obviously).\nI try to keep both projects consistent together, and will eventually port new developments made on \none project to the other one.\n\n\n# Changelog\n\n# v0.5.2\n\n**Bugfixes**\n\n- data-diff:\n  - Fix export of HTML report not working in cluster mode.\n\n# ~~v0.5.1~~ \n\nPlease do not use.\n\n# v0.5.0\n\n**New features:**\n\n- data-diff:\n  - Full sample rows in data-diff: in the data-diff HTML report, you can now click on a most frequent \n    value or change for a column and it will display the full content of a row where this change happens.\n    \n\n**Breaking Changes:**\n\n- data-diff:\n  - The names of the keys of the `DiffResult.diff_df_shards` dict have changed: \n    All keys except the root key (`""`) have been appended a REPETITION_MARKER (`"!"`).\n    This will make future manipulations easier. This should not impact users as it is a very advanced mechanic. \n\n\n\n# v0.4.0\n\nFixes and improvements on data_diff.\n\nImprovements:\n- data-diff: \n  - Now supports complex data types. Declaring a repeated field (e.g. `"s!.id"` in join_cols will now explode the\n    corresponding array and perform the diff on it).\n  - When columns are removed or renamed, they are now still displayed in the per-column diff report.\n  - Refactored and improved the HTML report: it is now fully standalone and can be opened without any \n    internet connection .\n  - Can now generate the HTML report directly on any remote file system accessible by Spark (e.g. "hdfs", "s3", etc.)\n  - A user-friendly error is now raised when one of the `join_cols` does not exist. \n- added package `spark_frame.filesystem` that can be used to read and write files directly from the driver using\n  the java FileSystem from Spark\'s JVM.\n\n**Breaking Changes:**\n- data-diff:\n  - `spark_frame.data_diff.DataframeComparator` object has been removed. \n    Please use directly the method `spark_frame.data_diff.compare_dataframes`.\n  - package `spark_frame.data_diff.diff_results` has been renamed to `diff_result`.\n  - Generating HTML reports for data diff does not require jinja anymore, but it does now require the installation \n    of the library [data-diff-viewer](https://pypi.org/project/data-diff-viewer/), \n    please check the [Compatibilities and requirements](#compatibilities-and-requirements) \n    section to know which version to use.\n  - The DiffResult object returned by the `compare_dataframes` method has evolved. In particular, the\n    type of `diff_df_shards` changed from a single `DataFrame` to a `Dict[str, DataFrame]`.\n  - `DiffFormatOptions.max_string_length` option has been removed\n  - `DiffFormatOptions.nb_diffed_rows` has been renamed to `nb_top_values_kept_per_column`\n  - `spark_frame.data_diff.compare_dataframes_impl.DataframeComparatorException` was replaced with\n    `spark_frame.exceptions.DataFrameComparisonException`\n  - `spark_frame.data_diff.compare_dataframes_impl.CombinatorialExplosionError` was replaced with\n    `spark_frame.exceptions.CombinatorialExplosionError`\n\nQA:\n- Spark: Added tests to ensure compatibility with Pyspark versions 3.3, 3.4 and 3.5\n- Replaced flake and isort with ruff\n\n# v0.3.2\n\nFixes and improvements on data_diff\n\n- Fix: automatic detection of join_col was sometimes selecting the wrong column\n- Visual improvements to HTML diff report:\n  - Name of columns used for join are now displayed in bold\n  - Total number of column is now displayed when the diff is ok\n  - Fix incorrect HTML diff display when one of the DataFrames is empty\n\n# v0.3.1\n\nFixes and improvements on data_diff\n\n- The `export_html_diff_report` method now accepts arguments to specify the path and encoding of the output html report. \n- Data-diff join now works correctly with null values\n- Visual improvements to HTML diff report\n\n\n# v0.3.0\n\nFixes and improvements on data_diff\n\n- Fixed incorrect diff results\n- Column values are not truncated at all, this was causing incorrect results. The possibility to limit the size \n  of the column values will be added back in a later version\n- Made sure that the most frequent values per column are now displayed by decreasing order of frequency\n\n\n# v0.2.0\n\nTwo new exciting features: *analyze* and *data_diff*. \nThey are still in experimental stage and will be improved in future releases.\n\n- Added a new transformation `spark_frame.transformations.analyze`.\n- Added new *data_diff* feature. Example:\n\n```python\nfrom pyspark.sql import DataFrame\nfrom spark_frame.data_diff import DataframeComparator\ndf1: DataFrame = ...\ndf2: DataFrame = ...\ndiff_result = DataframeComparator().compare_df(df1, df2) # Produces a DiffResult object\ndiff_result.display() # Print a diff report in the terminal\ndiff_result.export_to_html() # Generates a html diff report file named diff_report.html\n```\n\n\n# v0.1.1\n\n- Added a new transformation `spark_frame.transformations.flatten_all_arrays`.\n- Added support for multi-arg transformation to `nested.select` and `nested.with_fields` \n  With this feature, we can now access parent fields from higher levels\n  when applying a transformation. Example:\n  \n```\n>>> nested.print_schema(df)\n"""\nroot\n |-- id: integer (nullable = false)\n |-- s1!.average: integer (nullable = false)\n |-- s1!.values!: integer (nullable = false)\n"""\n>>> df.show(truncate=False)\n+---+--------------------------------------+\n|id |s1                                    |\n+---+--------------------------------------+\n|1  |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]|\n+---+--------------------------------------+\n>>> new_df = df.transform(nested.with_fields, {\n>>>     "s1!.values!": lambda s1, value: value - s1["average"]  # This transformation takes 2 arguments\n>>> })\n+---+-----------------------------------------+\n|id |s1                                       |\n+---+-----------------------------------------+\n|1  |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]|\n+---+-----------------------------------------+\n```\n\n# v0.1.0\n\n- Added a new _amazing_ module called `spark_frame.nested`, \n  which makes manipulation of nested data structure much easier!\n  Make sure to check out the [reference](https://furcypin.github.io/spark-frame/reference/nested/)\n  and the [use-cases](https://furcypin.github.io/spark-frame/use_cases/working_with_nested_data/).\n\n- Also added a new module called `spark_frame.nested_functions`,\n  which contains aggregation methods for nested data structures\n  ([See Reference](https://furcypin.github.io/spark-frame/reference/nested_functions/)).\n\n- New [transformations](https://furcypin.github.io/spark-frame/reference/transformations/):\n  - `spark_frame.transformations.transform_all_field_names`\n  - `spark_frame.transformations.transform_all_fields`\n  - `spark_frame.transformations.unnest_field`\n  - `spark_frame.transformations.unnest_all_fields`\n  - `spark_frame.transformations.union_dataframes`\n\n# v0.0.3\n\n- New transformation: `spark_frame.transformations.convert_all_maps_to_arrays`.\n- New transformation: `spark_frame.transformations.sort_all_arrays`.\n- New transformation: `spark_frame.transformations.harmonize_dataframes`.\n',
    'author': 'FurcyPin',
    'author_email': 'None',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'https://github.com/FurcyPin/spark-frame',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.8.1,<3.13',
}


setup(**setup_kwargs)
